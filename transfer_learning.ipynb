{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "transfer_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMxdTQmNrv1BhTXRDAqpCUr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gcheoyh/toy-transfer-learning-eg/blob/main/transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G2FRqZUKWN9"
      },
      "source": [
        "# Train MNIST Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er2Z_9Gb1xUa"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import layers, models, Model, optimizers, losses, Input\r\n",
        "from tensorflow.keras.backend import clear_session\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from os.path import exists, dirname, join\r\n",
        "import cv2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdtQpLyDU60V"
      },
      "source": [
        "mnist_train, mnist_test = tfds.load('mnist', split=['train', 'test'], shuffle_files=False, as_supervised=True)\r\n",
        "\r\n",
        "# convert to numpy format\r\n",
        "mnist_x_train = np.stack(list(map(lambda x: x[0].numpy(), mnist_train)), axis=0)\r\n",
        "mnist_y_train = np.stack(list(map(lambda x: x[1].numpy(), mnist_train)), axis=0)\r\n",
        "\r\n",
        "mnist_x_test = np.stack(list(map(lambda x: x[0].numpy(), mnist_test)), axis=0)\r\n",
        "mnist_y_test = np.stack(list(map(lambda x: x[1].numpy(), mnist_test)), axis=0)\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QHkTK1h7PXf"
      },
      "source": [
        "# train-val-test split\r\n",
        "train_size = int(mnist_x_train.shape[0] * 0.8)\r\n",
        "mnist_x_val = mnist_x_train[train_size:, :, :, :]\r\n",
        "mnist_y_val = mnist_y_train[train_size:]\r\n",
        "mnist_x_train = mnist_x_train[:train_size, :, :, :]\r\n",
        "mnist_y_train = mnist_y_train[:train_size]\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "dkREsPuI39Q-",
        "outputId": "26895962-077b-4451-8158-3c7efd352df2"
      },
      "source": [
        "# check data\r\n",
        "print(f'mnist_x_train: {mnist_x_train.shape}, mnist_x_val: {mnist_x_val.shape}, mnist_x_test: {mnist_x_test.shape}')\r\n",
        "print(f'mnist_y_train: {mnist_y_train.shape}, mnist_y_val: {mnist_y_val.shape}, mnist_y_test: {mnist_y_test.shape}')\r\n",
        "\r\n",
        "# display single image from x_train and x_test\r\n",
        "plt.figure()\r\n",
        "plt.imshow(mnist_x_train[0, :, :, :].reshape(28, 28), cmap='gray')\r\n",
        "plt.axis('off')\r\n",
        "plt.title(f'Label: {mnist_y_train[0]}')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "plt.imshow(mnist_x_test[0, :, :, :].reshape(28, 28), cmap='gray')\r\n",
        "plt.axis('off')\r\n",
        "plt.title(f'Label: {mnist_y_test[0]}')\r\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mnist_x_train: (48000, 28, 28, 1), mnist_x_val: (12000, 28, 28, 1), mnist_x_test: (10000, 28, 28, 1)\n",
            "mnist_y_train: (48000,), mnist_y_val: (12000,), mnist_y_test: (10000,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7S6sEIJKBZ5O",
        "outputId": "1c29ffcb-e707-49ff-dc38-4389bf014f5a"
      },
      "source": [
        "# simple model\r\n",
        "cnn = models.Sequential()\r\n",
        "cnn.add(layers.Conv2D(filters=32, kernel_size=3, input_shape=(28, 28, 1))) # conv first layer\r\n",
        "cnn.add(layers.BatchNormalization())\r\n",
        "cnn.add(layers.MaxPooling2D((2,2)))\r\n",
        "cnn.add(layers.Conv2D(filters=64, kernel_size=3, input_shape=(28, 28, 1))) # conv first layer\r\n",
        "cnn.add(layers.BatchNormalization())\r\n",
        "cnn.add(layers.MaxPooling2D((2,2)))\r\n",
        "cnn.add(layers.Flatten())\r\n",
        "cnn.add(layers.Dense(1024, activation='relu'))\r\n",
        "cnn.add(layers.BatchNormalization())\r\n",
        "cnn.add(layers.Dropout(0.3))\r\n",
        "cnn.add(layers.Dense(256, activation='relu'))\r\n",
        "cnn.add(layers.BatchNormalization())\r\n",
        "cnn.add(layers.Dropout(0.3))\r\n",
        "cnn.add(layers.Dense(10, activation='softmax'))\r\n",
        "\r\n",
        "print(cnn.summary())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 26, 26, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 11, 11, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              1639424   \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 1,928,714\n",
            "Trainable params: 1,925,962\n",
            "Non-trainable params: 2,752\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMXvgluLDJoJ"
      },
      "source": [
        "# compile model\r\n",
        "opt = optimizers.Adam(learning_rate=1e-4, beta_1=0.9, beta_2=0.999)\r\n",
        "loss = losses.SparseCategoricalCrossentropy(from_logits=False) # using softmax layer\r\n",
        "cnn.compile(optimizer=opt, loss=loss, metrics=['accuracy'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3MGHtFs82Dv",
        "outputId": "d43a99b7-7f7b-4214-a7f5-8e626848f9a0"
      },
      "source": [
        "# mount drive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "# change directory\r\n",
        "% cd /content/drive/MyDrive/Colab\\ Notebooks/misc/toy-transfer-learning-eg\r\n",
        "! pwd\r\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Colab Notebooks/misc/toy-transfer-learning-eg\n",
            "/content/drive/MyDrive/Colab Notebooks/misc/toy-transfer-learning-eg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Jzt2pnrDRH-",
        "outputId": "ef04e5e0-729f-4e3c-c50c-5c0fb2949898"
      },
      "source": [
        "MNIST_MODEL_PATH = '/content/drive/MyDrive/Colab Notebooks/misc/toy-transfer-learning-eg/mnist_classifier.h5'\r\n",
        "\r\n",
        "if exists(MNIST_MODEL_PATH):\r\n",
        "    # load model if possible\r\n",
        "    cnn = models.load_model(MNIST_MODEL_PATH)\r\n",
        "\r\n",
        "else:\r\n",
        "\r\n",
        "    # train model with early stopping on val loss\r\n",
        "    print('Training model')\r\n",
        "    callback = EarlyStopping(monitor='val_loss', patience=3, mode='auto', restore_best_weights=True)\r\n",
        "    hist = cnn.fit(mnist_x_train, mnist_y_train, batch_size=32, epochs=30,\r\n",
        "                   validation_data = (mnist_x_val, mnist_y_val), callbacks=[callback])\r\n",
        "    \r\n",
        "    # save model\r\n",
        "    cnn.save(MNIST_MODEL_PATH)\r\n",
        "\r\n",
        "# test on test set to make sure it works\r\n",
        "print('\\nTesting model')\r\n",
        "results = cnn.evaluate(mnist_x_test, mnist_y_test, batch_size=128)\r\n",
        "print(f'Test loss: {results[0]:.4f}, test acc: {results[1]:.4f}')\r\n",
        "\r\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Testing model\n",
            "79/79 [==============================] - 2s 3ms/step - loss: 0.0330 - accuracy: 0.9902\n",
            "Test loss: 0.0330, test acc: 0.9902\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rp6d_NZTFbV"
      },
      "source": [
        "# Retrieve EMNIST Letters Dataset and Perform Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F6q9zlDTH9k"
      },
      "source": [
        "letters_train, letters_test = tfds.load('emnist/letters', split=['train', 'test'], shuffle_files=False, as_supervised=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaNLnJI-P05q"
      },
      "source": [
        "# convert to numpy format\r\n",
        "letters_x_train = np.stack(list(map(lambda x: x[0].numpy().T.reshape(28, 28, 1), letters_train)), axis=0)\r\n",
        "letters_y_train = np.stack(list(map(lambda x: x[1].numpy(), letters_train)), axis=0)\r\n",
        "letters_y_train -= 1  # zero index the labels\r\n",
        "\r\n",
        "# artificially limit size of training set to show benefits of fine tuning\r\n",
        "max_size = 5000\r\n",
        "letters_x_train = letters_x_train[:max_size, :]\r\n",
        "letters_y_train = letters_y_train[:max_size]\r\n",
        "\r\n",
        "letters_x_test = np.stack(list(map(lambda x: x[0].numpy().T.reshape(28, 28, 1), letters_test)), axis=0)\r\n",
        "letters_y_test = np.stack(list(map(lambda x: x[1].numpy(), letters_test)), axis=0)\r\n",
        "letters_y_test -= 1  # zero index the labels\r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqZqPNyXRbkw"
      },
      "source": [
        "# train test split for letters\r\n",
        "train_size = int(letters_x_train.shape[0] * 0.8)\r\n",
        "letters_x_val = letters_x_train[train_size:, :]\r\n",
        "letters_y_val = letters_y_train[train_size:]\r\n",
        "\r\n",
        "letters_x_train = letters_x_train[:train_size, :] \r\n",
        "letters_y_train = letters_y_train[:train_size]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "tfXALGCUjCSR",
        "outputId": "a52414c4-d6f4-4b16-d053-a510eb485db8"
      },
      "source": [
        "# check data\r\n",
        "print(f'letters_x_train: {letters_x_train.shape}, letters_x_val: {letters_x_val.shape}, letters_x_test: {letters_x_test.shape}')\r\n",
        "print(f'letters_y_train: {letters_y_train.shape}, letters_y_val: {letters_y_val.shape}, letters_y_test: {letters_y_test.shape}')\r\n",
        "\r\n",
        "# display single image from x_train and x_test\r\n",
        "plt.figure()\r\n",
        "plt.imshow(letters_x_train[0, :, :, :].reshape(28, 28), cmap='gray')\r\n",
        "plt.axis('off')\r\n",
        "plt.title(f'Label: {letters_y_train[0]}')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "plt.imshow(letters_x_test[0, :, :, :].reshape(28, 28), cmap='gray')\r\n",
        "plt.axis('off')\r\n",
        "plt.title(f'Label: {letters_y_test[0]}')\r\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "letters_x_train: (4000, 28, 28, 1), letters_x_val: (1000, 28, 28, 1), letters_x_test: (14800, 28, 28, 1)\n",
            "letters_y_train: (4000,), letters_y_val: (1000,), letters_y_test: (14800,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ck2k-vYso35x",
        "outputId": "acc1fb7f-dbff-42cd-cddc-25572cb8879e"
      },
      "source": [
        "letters_cnn = models.Sequential(cnn.layers[:-1])\r\n",
        "letters_cnn.add(layers.Dense(26, activation='softmax')) # add new softmax layer\r\n",
        "print(letters_cnn.summary())\r\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 26, 26, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 11, 11, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1024)              1639424   \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 26)                6682      \n",
            "=================================================================\n",
            "Total params: 1,932,826\n",
            "Trainable params: 1,930,074\n",
            "Non-trainable params: 2,752\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UjAbeV2tz2c",
        "outputId": "0714e7fc-5b4c-4010-af5b-c3836dc548d3"
      },
      "source": [
        "# freeze convolutional layers\r\n",
        "# not advisable to freeze batch normalization due to different batch statistics (different domain)\r\n",
        "# https://forums.fast.ai/t/why-are-batchnorm-layers-set-to-trainable-in-a-frozen-model/46560\r\n",
        "for layer in letters_cnn.layers:\r\n",
        "    name = layer.name\r\n",
        "    if 'conv2d' in name:\r\n",
        "        layer.trainable = False\r\n",
        "\r\n",
        "for layer in letters_cnn.layers:\r\n",
        "    print(f'Name:{layer.name}, trainable status:{layer.trainable}')\r\n",
        "\r\n",
        "# compile model\r\n",
        "letters_cnn.compile(optimizer=opt, loss=loss, metrics=['accuracy'])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name:conv2d_4, trainable status:False\n",
            "Name:batch_normalization_7, trainable status:True\n",
            "Name:max_pooling2d_4, trainable status:True\n",
            "Name:conv2d_5, trainable status:False\n",
            "Name:batch_normalization_8, trainable status:True\n",
            "Name:max_pooling2d_5, trainable status:True\n",
            "Name:flatten_2, trainable status:True\n",
            "Name:dense_6, trainable status:True\n",
            "Name:batch_normalization_9, trainable status:True\n",
            "Name:dropout_3, trainable status:True\n",
            "Name:dense_7, trainable status:True\n",
            "Name:batch_normalization_10, trainable status:True\n",
            "Name:dropout_4, trainable status:True\n",
            "Name:dense_3, trainable status:True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Zrws-9Ywntx",
        "outputId": "4c8bec37-a528-4a92-dc98-7decd88c075c"
      },
      "source": [
        "# finetune network!\r\n",
        "print('Finetuning letters model')\r\n",
        "callback = EarlyStopping(monitor='val_loss', patience=3, mode='auto', restore_best_weights=True)\r\n",
        "letters_hist = letters_cnn.fit(letters_x_train, letters_y_train, batch_size=32, epochs=30,\r\n",
        "                       validation_data = (letters_x_val, letters_y_val), callbacks=[callback])\r\n",
        "\r\n",
        "# save model\r\n",
        "# LETTERS_MODEL_PATH = join(dirname(MNIST_MODEL_PATH), 'letters_classifier.h5')\r\n",
        "# letters_cnn.save(LETTERS_MODEL_PATH)\r\n",
        "\r\n",
        "# test on test set to make sure it works\r\n",
        "print('\\nTesting letters model')\r\n",
        "results = letters_cnn.evaluate(letters_x_test, letters_y_test, batch_size=128)\r\n",
        "print(f'Test loss: {results[0]:.4f}, test acc: {results[1]:.4f}')\r\n",
        "\r\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finetuning letters model\n",
            "Epoch 1/30\n",
            "125/125 [==============================] - 2s 8ms/step - loss: 3.2922 - accuracy: 0.1772 - val_loss: 1.1857 - val_accuracy: 0.6740\n",
            "Epoch 2/30\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 1.3393 - accuracy: 0.6088 - val_loss: 0.7972 - val_accuracy: 0.7700\n",
            "Epoch 3/30\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.9546 - accuracy: 0.7163 - val_loss: 0.6730 - val_accuracy: 0.8020\n",
            "Epoch 4/30\n",
            "125/125 [==============================] - 0s 4ms/step - loss: 0.7808 - accuracy: 0.7768 - val_loss: 0.6058 - val_accuracy: 0.8220\n",
            "Epoch 5/30\n",
            "125/125 [==============================] - 0s 4ms/step - loss: 0.6261 - accuracy: 0.8144 - val_loss: 0.5529 - val_accuracy: 0.8350\n",
            "Epoch 6/30\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.5438 - accuracy: 0.8320 - val_loss: 0.5209 - val_accuracy: 0.8320\n",
            "Epoch 7/30\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.4678 - accuracy: 0.8608 - val_loss: 0.5076 - val_accuracy: 0.8360\n",
            "Epoch 8/30\n",
            "125/125 [==============================] - 0s 4ms/step - loss: 0.3994 - accuracy: 0.8876 - val_loss: 0.4927 - val_accuracy: 0.8460\n",
            "Epoch 9/30\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.3781 - accuracy: 0.8861 - val_loss: 0.4782 - val_accuracy: 0.8500\n",
            "Epoch 10/30\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.3335 - accuracy: 0.9041 - val_loss: 0.4560 - val_accuracy: 0.8610\n",
            "Epoch 11/30\n",
            "125/125 [==============================] - 0s 4ms/step - loss: 0.2821 - accuracy: 0.9150 - val_loss: 0.4451 - val_accuracy: 0.8630\n",
            "Epoch 12/30\n",
            "125/125 [==============================] - 0s 4ms/step - loss: 0.2870 - accuracy: 0.9103 - val_loss: 0.4490 - val_accuracy: 0.8610\n",
            "Epoch 13/30\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2527 - accuracy: 0.9230 - val_loss: 0.4399 - val_accuracy: 0.8680\n",
            "Epoch 14/30\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2329 - accuracy: 0.9331 - val_loss: 0.4617 - val_accuracy: 0.8570\n",
            "Epoch 15/30\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2006 - accuracy: 0.9430 - val_loss: 0.4416 - val_accuracy: 0.8600\n",
            "Epoch 16/30\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.1914 - accuracy: 0.9408 - val_loss: 0.4391 - val_accuracy: 0.8640\n",
            "Epoch 17/30\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.1711 - accuracy: 0.9513 - val_loss: 0.4379 - val_accuracy: 0.8590\n",
            "Epoch 18/30\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.1680 - accuracy: 0.9583 - val_loss: 0.4391 - val_accuracy: 0.8570\n",
            "Epoch 19/30\n",
            "125/125 [==============================] - 0s 4ms/step - loss: 0.1584 - accuracy: 0.9543 - val_loss: 0.4298 - val_accuracy: 0.8650\n",
            "Epoch 20/30\n",
            "125/125 [==============================] - 0s 4ms/step - loss: 0.1339 - accuracy: 0.9653 - val_loss: 0.4402 - val_accuracy: 0.8680\n",
            "Epoch 21/30\n",
            "125/125 [==============================] - 0s 4ms/step - loss: 0.1254 - accuracy: 0.9696 - val_loss: 0.4367 - val_accuracy: 0.8650\n",
            "Epoch 22/30\n",
            "125/125 [==============================] - 0s 4ms/step - loss: 0.1228 - accuracy: 0.9706 - val_loss: 0.4311 - val_accuracy: 0.8680\n",
            "\n",
            "Testing letters model\n",
            "116/116 [==============================] - 0s 3ms/step - loss: 0.4794 - accuracy: 0.8485\n",
            "Test loss: 0.4794, test acc: 0.8485\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyv2vv1xydRa",
        "outputId": "921f3815-12d2-4560-e3a7-8b312905162e"
      },
      "source": [
        "# compare this to training the same model from scratch\r\n",
        "new_let_cnn = models.Sequential()\r\n",
        "new_let_cnn.add(layers.Conv2D(filters=32, kernel_size=3, input_shape=(28, 28, 1))) # conv first layer\r\n",
        "new_let_cnn.add(layers.BatchNormalization())\r\n",
        "new_let_cnn.add(layers.MaxPooling2D((2,2)))\r\n",
        "new_let_cnn.add(layers.Conv2D(filters=64, kernel_size=3, input_shape=(28, 28, 1))) # conv first layer\r\n",
        "new_let_cnn.add(layers.BatchNormalization())\r\n",
        "new_let_cnn.add(layers.MaxPooling2D((2,2)))\r\n",
        "new_let_cnn.add(layers.Flatten())\r\n",
        "new_let_cnn.add(layers.Dense(1024, activation='relu'))\r\n",
        "new_let_cnn.add(layers.BatchNormalization())\r\n",
        "new_let_cnn.add(layers.Dropout(0.3))\r\n",
        "new_let_cnn.add(layers.Dense(256, activation='relu'))\r\n",
        "new_let_cnn.add(layers.BatchNormalization())\r\n",
        "new_let_cnn.add(layers.Dropout(0.3))\r\n",
        "new_let_cnn.add(layers.Dense(26, activation='softmax'))\r\n",
        "\r\n",
        "print(new_let_cnn.summary())\r\n",
        "\r\n",
        "new_let_cnn.compile(optimizer=opt, loss=loss, metrics=['accuracy'])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 26, 26, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 11, 11, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1024)              1639424   \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 26)                6682      \n",
            "=================================================================\n",
            "Total params: 1,932,826\n",
            "Trainable params: 1,930,074\n",
            "Non-trainable params: 2,752\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Uga4QTCyzB_",
        "outputId": "9e7a728e-ab17-439f-ed8d-dc99988d8036"
      },
      "source": [
        "# train network from scratch!\r\n",
        "print('Training letters model from scratch')\r\n",
        "callback = EarlyStopping(monitor='val_loss', patience=3, mode='auto', restore_best_weights=True)\r\n",
        "new_let_hist = new_let_cnn.fit(letters_x_train, letters_y_train, batch_size=32, epochs=30,\r\n",
        "                               validation_data = (letters_x_val, letters_y_val), callbacks=[callback])\r\n",
        "\r\n",
        "# save model\r\n",
        "# NEW_LETTERS_MODEL_PATH = join(dirname(MNIST_MODEL_PATH), 'letters_fromscratch_classifier.h5')\r\n",
        "# new_let_cnn.save(NEW_LETTERS_MODEL_PATH)\r\n",
        "\r\n",
        "# test on test set to make sure it works\r\n",
        "print('\\nTesting letters model')\r\n",
        "results = new_let_cnn.evaluate(letters_x_test, letters_y_test, batch_size=128)\r\n",
        "print(f'Test loss: {results[0]:.4f}, test acc: {results[1]:.4f}')\r\n",
        "\r\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training letters model from scratch\n",
            "Epoch 1/30\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 2.2502 - accuracy: 0.4152 - val_loss: 1.8180 - val_accuracy: 0.5320\n",
            "Epoch 2/30\n",
            "125/125 [==============================] - 0s 4ms/step - loss: 0.8773 - accuracy: 0.7367 - val_loss: 0.7282 - val_accuracy: 0.7750\n",
            "Epoch 3/30\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.6549 - accuracy: 0.7992 - val_loss: 0.6439 - val_accuracy: 0.7910\n",
            "Epoch 4/30\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.5350 - accuracy: 0.8388 - val_loss: 0.6032 - val_accuracy: 0.8230\n",
            "Epoch 5/30\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.4214 - accuracy: 0.8686 - val_loss: 0.5456 - val_accuracy: 0.8370\n",
            "Epoch 6/30\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.3855 - accuracy: 0.8923 - val_loss: 0.5316 - val_accuracy: 0.8340\n",
            "Epoch 7/30\n",
            "125/125 [==============================] - 0s 4ms/step - loss: 0.3110 - accuracy: 0.9063 - val_loss: 0.5153 - val_accuracy: 0.8370\n",
            "Epoch 8/30\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2897 - accuracy: 0.9195 - val_loss: 0.4963 - val_accuracy: 0.8370\n",
            "Epoch 9/30\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2387 - accuracy: 0.9321 - val_loss: 0.4927 - val_accuracy: 0.8450\n",
            "Epoch 10/30\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2212 - accuracy: 0.9369 - val_loss: 0.4969 - val_accuracy: 0.8450\n",
            "Epoch 11/30\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1982 - accuracy: 0.9473 - val_loss: 0.4915 - val_accuracy: 0.8480\n",
            "Epoch 12/30\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1829 - accuracy: 0.9530 - val_loss: 0.4827 - val_accuracy: 0.8470\n",
            "Epoch 13/30\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.1628 - accuracy: 0.9591 - val_loss: 0.4940 - val_accuracy: 0.8450\n",
            "Epoch 14/30\n",
            "125/125 [==============================] - 0s 4ms/step - loss: 0.1487 - accuracy: 0.9618 - val_loss: 0.4739 - val_accuracy: 0.8510\n",
            "Epoch 15/30\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.1335 - accuracy: 0.9650 - val_loss: 0.4837 - val_accuracy: 0.8460\n",
            "Epoch 16/30\n",
            "125/125 [==============================] - 0s 4ms/step - loss: 0.1266 - accuracy: 0.9700 - val_loss: 0.4690 - val_accuracy: 0.8530\n",
            "Epoch 17/30\n",
            "125/125 [==============================] - 0s 4ms/step - loss: 0.1205 - accuracy: 0.9695 - val_loss: 0.4844 - val_accuracy: 0.8490\n",
            "Epoch 18/30\n",
            "125/125 [==============================] - 0s 4ms/step - loss: 0.1003 - accuracy: 0.9763 - val_loss: 0.4969 - val_accuracy: 0.8420\n",
            "Epoch 19/30\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.1105 - accuracy: 0.9698 - val_loss: 0.4929 - val_accuracy: 0.8440\n",
            "\n",
            "Testing letters model\n",
            "116/116 [==============================] - 0s 2ms/step - loss: 0.5250 - accuracy: 0.8382\n",
            "Test loss: 0.5250, test acc: 0.8382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3hufhUB3_Od"
      },
      "source": [
        "We see that finetuning offers minimal improvement over training the model from scratch, especially when limited data is available from the target domain of interest (in this case, MNIST Letters)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh07o-nSa7XZ"
      },
      "source": [
        "We can also consider using finetuning for more complex tasks (e.g. CIFAR10), and using more exensive networks (e.g. ResNet50). We compare classification on the CIFAR10 set, using a ResNet with weights initialized from ImageNet training, and another ResNet with random weight initialization. Note that no weights are frozen in this case. This works well since it provides more flexibility in fine-tuning, leading to better classification rates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3C8V8ot5doO"
      },
      "source": [
        "# Finetuning ResNet50 for CIFAR10 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kx46G_z9F7J8"
      },
      "source": [
        "# import cifar10 dataset\r\n",
        "cifar10_train, cifar10_test = tfds.load('cifar10', split=['train', 'test'], shuffle_files=False, as_supervised=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLmJ-3sBGRza"
      },
      "source": [
        "cifar10_x_train = np.stack(list(map(lambda x: x[0].numpy(), cifar10_train)), axis=0)\r\n",
        "cifar10_y_train = np.stack(list(map(lambda x: x[1].numpy(), cifar10_train)), axis=0)\r\n",
        "\r\n",
        "cifar10_x_test = np.stack(list(map(lambda x: x[0].numpy(), cifar10_test)), axis=0)\r\n",
        "cifar10_y_test = np.stack(list(map(lambda x: x[1].numpy(), cifar10_test)), axis=0)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IdnfTF5JkXB"
      },
      "source": [
        "# train test split\r\n",
        "train_size = int(cifar10_x_train.shape[0] * 0.8)\r\n",
        "cifar10_x_val = cifar10_x_train[train_size:]\r\n",
        "cifar10_y_val = cifar10_y_train[train_size:]\r\n",
        "\r\n",
        "cifar10_x_train = cifar10_x_train[:train_size]\r\n",
        "cifar10_y_train = cifar10_y_train[:train_size]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9fTQ1_EKYXe",
        "outputId": "5305b9f2-586a-4104-87e1-5b59cd3a000f"
      },
      "source": [
        "print(f'cifar10_x_train: {cifar10_x_train.shape}, cifar10_x_val: {cifar10_x_val.shape}, cifar10_x_test: {cifar10_x_test.shape}')\r\n",
        "print(f'cifar10_y_train: {cifar10_y_train.shape}, cifar10_y_val: {cifar10_y_val.shape}, cifar10_y_test: {cifar10_y_test.shape}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cifar10_x_train: (40000, 32, 32, 3), cifar10_x_val: (10000, 32, 32, 3), cifar10_x_test: (10000, 32, 32, 3)\n",
            "cifar10_y_train: (40000,), cifar10_y_val: (10000,), cifar10_y_test: (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v65htZ6O5vvK",
        "outputId": "47c050af-51cb-4a65-f546-d8a8c7f6f0cd"
      },
      "source": [
        "from tensorflow.keras.applications import ResNet50\r\n",
        "\r\n",
        "cifar_input = Input(shape=(32, 32, 3))\r\n",
        "resnet = ResNet50(include_top=False, weights='imagenet', input_tensor=cifar_input)\r\n",
        "\r\n",
        "print(resnet.summary())\r\n",
        "\r\n",
        "feat = layers.Flatten()(resnet(cifar_input))\r\n",
        "out = layers.Dense(10, activation='softmax')(feat)\r\n",
        "\r\n",
        "resnet_mod = Model(inputs=cifar_input, outputs=out)\r\n",
        "print('\\n\\n')\r\n",
        "print(resnet_mod.summary())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 23,587,712\n",
            "Trainable params: 23,534,592\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "\n",
            "\n",
            "\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "resnet50 (Functional)        (None, 1, 1, 2048)        23587712  \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 23,608,202\n",
            "Trainable params: 23,555,082\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFsIYJTx6Qlz",
        "outputId": "b86a664c-36c4-4d7a-9725-3ee86ed81742"
      },
      "source": [
        "# compile and train model\r\n",
        "print('Finetuning ResNet Pretrained Model')\r\n",
        "resnet_mod.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\r\n",
        "callback = EarlyStopping(monitor='val_loss', patience=10, mode='auto', restore_best_weights=True)\r\n",
        "resnet_letters_hist = resnet_mod.fit(cifar10_x_train, cifar10_y_train, batch_size=32, epochs=50,\r\n",
        "                                      validation_data = (cifar10_x_val, cifar10_y_val), callbacks=[callback])\r\n",
        "\r\n",
        "# test on test set to make sure it works\r\n",
        "print('\\nTesting cifar10 model')\r\n",
        "results = resnet_mod.evaluate(cifar10_x_test, cifar10_y_test, batch_size=128)\r\n",
        "print(f'Test loss: {results[0]:.4f}, test acc: {results[1]:.4f}')\r\n",
        "\r\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finetuning ResNet Pretrained Model\n",
            "Epoch 1/50\n",
            "1250/1250 [==============================] - 51s 38ms/step - loss: 1.5781 - accuracy: 0.5104 - val_loss: 1.0362 - val_accuracy: 0.6733\n",
            "Epoch 2/50\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.7483 - accuracy: 0.7488 - val_loss: 0.6642 - val_accuracy: 0.7705\n",
            "Epoch 3/50\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.5619 - accuracy: 0.8106 - val_loss: 0.6628 - val_accuracy: 0.7831\n",
            "Epoch 4/50\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.4257 - accuracy: 0.8598 - val_loss: 0.6541 - val_accuracy: 0.7843\n",
            "Epoch 5/50\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.3568 - accuracy: 0.8804 - val_loss: 0.6053 - val_accuracy: 0.8028\n",
            "Epoch 6/50\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.2747 - accuracy: 0.9114 - val_loss: 0.6329 - val_accuracy: 0.8086\n",
            "Epoch 7/50\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.1713 - accuracy: 0.9412 - val_loss: 0.7554 - val_accuracy: 0.7882\n",
            "Epoch 8/50\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.1671 - accuracy: 0.9452 - val_loss: 1.0662 - val_accuracy: 0.7563\n",
            "Epoch 9/50\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.1904 - accuracy: 0.9348 - val_loss: 1.7484 - val_accuracy: 0.7575\n",
            "Epoch 10/50\n",
            "1250/1250 [==============================] - 47s 37ms/step - loss: 0.1353 - accuracy: 0.9570 - val_loss: 0.7978 - val_accuracy: 0.8026\n",
            "Epoch 11/50\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.0804 - accuracy: 0.9736 - val_loss: 0.7845 - val_accuracy: 0.8050\n",
            "Epoch 12/50\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.0812 - accuracy: 0.9734 - val_loss: 0.9464 - val_accuracy: 0.7946\n",
            "Epoch 13/50\n",
            "1250/1250 [==============================] - 47s 37ms/step - loss: 0.0815 - accuracy: 0.9736 - val_loss: 0.8229 - val_accuracy: 0.7956\n",
            "Epoch 14/50\n",
            "1250/1250 [==============================] - 47s 37ms/step - loss: 0.1080 - accuracy: 0.9652 - val_loss: 0.8580 - val_accuracy: 0.7999\n",
            "Epoch 15/50\n",
            "1250/1250 [==============================] - 47s 38ms/step - loss: 0.0567 - accuracy: 0.9813 - val_loss: 1.0956 - val_accuracy: 0.8013\n",
            "\n",
            "Testing cifar10 model\n",
            "79/79 [==============================] - 1s 15ms/step - loss: 0.6354 - accuracy: 0.8014\n",
            "Test loss: 0.6354, test acc: 0.8014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRfNj3ciRFsc",
        "outputId": "141663ae-4ad6-44f8-ffc7-a0ef90eafe24"
      },
      "source": [
        "# randomly initialize weights\r\n",
        "resnet_new = ResNet50(include_top=False, weights=None, input_tensor=cifar_input)\r\n",
        "\r\n",
        "feat = layers.Flatten()(resnet_new(cifar_input))\r\n",
        "out = layers.Dense(10, activation='softmax')(feat)\r\n",
        "\r\n",
        "resnet_mod_new = Model(inputs=cifar_input, outputs=out)\r\n",
        "print('\\n\\n')\r\n",
        "print(resnet_mod_new.summary())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "resnet50 (Functional)        (None, 1, 1, 2048)        23587712  \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 23,608,202\n",
            "Trainable params: 23,555,082\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DTRDlt9RhRU",
        "outputId": "31332ee3-ca5f-4da9-c9ba-5f60373c216c"
      },
      "source": [
        "# compile and train model\r\n",
        "print('Training ResNet Model from Scratch')\r\n",
        "resnet_mod_new.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\r\n",
        "callback = EarlyStopping(monitor='val_loss', patience=5, mode='auto', restore_best_weights=True)\r\n",
        "resnet_letters_hist = resnet_mod_new.fit(cifar10_x_train, cifar10_y_train, batch_size=32, epochs=50,\r\n",
        "                                      validation_data = (cifar10_x_val, cifar10_y_val), callbacks=[callback])\r\n",
        "\r\n",
        "# test on test set to make sure it works\r\n",
        "print('\\nTesting cifar10 model')\r\n",
        "results = resnet_mod_new.evaluate(cifar10_x_test, cifar10_y_test, batch_size=128)\r\n",
        "print(f'Test loss: {results[0]:.4f}, test acc: {results[1]:.4f}')\r\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training ResNet Model from Scratch\n",
            "Epoch 1/50\n",
            "1250/1250 [==============================] - 52s 39ms/step - loss: 2.2726 - accuracy: 0.2729 - val_loss: 1.7124 - val_accuracy: 0.4126\n",
            "Epoch 2/50\n",
            "1250/1250 [==============================] - 47s 37ms/step - loss: 1.5699 - accuracy: 0.4427 - val_loss: 1.6178 - val_accuracy: 0.4525\n",
            "Epoch 3/50\n",
            "1250/1250 [==============================] - 47s 38ms/step - loss: 1.3923 - accuracy: 0.5050 - val_loss: 1.5012 - val_accuracy: 0.4987\n",
            "Epoch 4/50\n",
            "1250/1250 [==============================] - 47s 38ms/step - loss: 1.2511 - accuracy: 0.5606 - val_loss: 1.4472 - val_accuracy: 0.5021\n",
            "Epoch 5/50\n",
            "1250/1250 [==============================] - 47s 37ms/step - loss: 1.1488 - accuracy: 0.5914 - val_loss: 1.3695 - val_accuracy: 0.5291\n",
            "Epoch 6/50\n",
            "1250/1250 [==============================] - 47s 38ms/step - loss: 1.0652 - accuracy: 0.6262 - val_loss: 3.3895 - val_accuracy: 0.2835\n",
            "Epoch 7/50\n",
            "1250/1250 [==============================] - 47s 37ms/step - loss: 1.0035 - accuracy: 0.6425 - val_loss: 1.4305 - val_accuracy: 0.5212\n",
            "Epoch 8/50\n",
            "1250/1250 [==============================] - 47s 38ms/step - loss: 0.8744 - accuracy: 0.6924 - val_loss: 1.3389 - val_accuracy: 0.5529\n",
            "Epoch 9/50\n",
            "1250/1250 [==============================] - 47s 37ms/step - loss: 0.8091 - accuracy: 0.7216 - val_loss: 1.2032 - val_accuracy: 0.6033\n",
            "Epoch 10/50\n",
            "1250/1250 [==============================] - 47s 37ms/step - loss: 0.6560 - accuracy: 0.7724 - val_loss: 1.3796 - val_accuracy: 0.5666\n",
            "Epoch 11/50\n",
            "1250/1250 [==============================] - 47s 37ms/step - loss: 0.5675 - accuracy: 0.8009 - val_loss: 1.7474 - val_accuracy: 0.5175\n",
            "Epoch 12/50\n",
            "1250/1250 [==============================] - 47s 37ms/step - loss: 0.5123 - accuracy: 0.8233 - val_loss: 1.2762 - val_accuracy: 0.6030\n",
            "Epoch 13/50\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.4063 - accuracy: 0.8589 - val_loss: 1.8696 - val_accuracy: 0.5376\n",
            "Epoch 14/50\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.4331 - accuracy: 0.8518 - val_loss: 1.4725 - val_accuracy: 0.6057\n",
            "\n",
            "Testing cifar10 model\n",
            "79/79 [==============================] - 1s 15ms/step - loss: 1.2298 - accuracy: 0.5962\n",
            "Test loss: 1.2298, test acc: 0.5962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bee_2dPbjvm"
      },
      "source": [
        "We see that the ResNet initialized with ImageNet weights significantly outperforms ResNet with random weight initialization, again pointing to the benefits of transfer learning. The performance gain in this case might be particularly large, due to the fact that the pretrained ResNet was trained on such an extensive dataset as ImageNet, allowing us to extract particularly robust features. "
      ]
    }
  ]
}